### SQLFrame  

https://github.com/eakmanrq/sqlframe
```
from sqlframe import activate
activate("duckdb")

from pyspark.sql import SparkSession
spark=SparkSession.builder.getOrCreate()
# spark is actually an SQLFrame DucDBSession
```

### Databricks System tables

https://www.youtube.com/watch?v=LcRWHzk8Wm4



https://medium.com/@lminhkhoa/speed-up-databricks-jobs-with-multi-threading-899a983c611a

```
dbldatagen, which is a Python Databricks Lab Project that allows us to generate dummy data inside Databricks for simulating, testing or building demos. Create some notebook cells:

%pip install dbldatagen

```


âœ¨They're all free! Whether you're new to AI or an experienced pro, these courses are a great way to master AI in 2024

No Payment requied âœ…

ğŸ”— 7000+ Course Free Access : https://lnkd.in/grzQeaZa   https://lnkd.in/efnsEvpF 

ğŸ”— Google Data Analytics
https://lnkd.in/g6zmN3C3

ğŸª¢ Google Project Management
https://lnkd.in/gfeFvw2J

ğŸ”— Google IT Support
https://lnkd.in/gCSYYfba

ğŸª¢ Google Digital Marketing & E-commerce
https://lnkd.in/gQj3s9WM

ğŸ”— Google IT Automation with Python
https://lnkd.in/gBGZV9WT

ğŸª¢ Google Business Intelligence
https://lnkd.in/g7UVQFSR

ğŸ”— Google Advanced Data Analytics
https://lnkd.in/ghMiGXKV

ğŸª¢ Google Cybersecurity
https://lnkd.in/gGMnr-BQ

ğŸ”— Meta Database Engineer
https://lnkd.in/gpPtb5Dv

ğŸª¢ IBM Data Science
https://lnkd.in/g3jMW-fQ

ğŸ”—Indigenous Canada
https://lnkd.in/gZqVeGtx

ğŸª¢ Machine Learning
https://lnkd.in/gShaPZQX

ğŸ”— IBM Data Analyst
https://lnkd.in/gtJ5Vn3S

ğŸª¢ Deep Learning
https://lnkd.in/gSj38xxC



https://www.linkedin.com/posts/abhisek-sahu-84a404b1_lakehouse-architecture-lakehouse-datawarehouse-activity-7225743778840506368-zaV5 

https://blog.det.life/exploring-advanced-open-data-formats-apache-hudi-apache-iceberg-and-delta-lake-eb764c161159

1. Snowflake vs Databricks - And the Battle For Iceberg
https://lnkd.in/gq-KCFsK

2. Common Pitfalls in Deploying Airflow for Data Teams
https://lnkd.in/gGepn-K5

3. Memory Efficient Data Streaming To Parquet Files
https://lnkd.in/gpVwKhEK

4. Why is Polars All The Rage? by Daniel Beach
https://lnkd.in/gUVATCj5

5. Apache Iceberg - What Is It by Julien Hurault
https://lnkd.in/gfntMYZX

6. The Data Engineerâ€™s Guide to CDC for Analytics, Ops, and AI Pipelines
https://lnkd.in/esbcDi_q

7. Data Modeling Where Theory Meets Reality
https://lnkd.in/g_8FCpuA

### Databricks  Certifications

https://www.linkedin.com/in/abhisek-sahu-84a404b1/

```
Databricks ğ‚ğğ«ğ­ğ¢ğŸğ¢ğğ ğƒğšğ­ğš ğ„ğ§ğ ğ¢ğ§ğğğ« ğ€ğ¬ğ¬ğ¨ğœğ¢ğšğ­ğ ğ„ğ±ğšğ¦ :


1. The Databricks Certified Data Engineer Associate certification exam assesses an individualâ€™s ability to use the Databricks Lakehouse Platform to complete introductory data engineering tasks. 

2. This includes an understanding of the Lakehouse Platform and its workspace, its architecture, and its capabilities.

3. It also assesses the ability to perform multi-hop architecture ETL tasks using Apache Sparkâ„¢ SQL and Python in both batch and incrementally processed paradigms. 

4. Finally, the exam assesses the testerâ€™s ability to put basic ETL pipelines and Databricks SQL queries and dashboards into production while maintaining entity permissions. 

ğ„ğ±ğšğ¦ ğ‚ğ¨ğ¯ğğ«ğ¬:

Databricks Lakehouse Platform â€“ 24%
ELT With Spark SQL and Python â€“ 29%
Incremental Data Processing â€“ 22%
Production Pipelines â€“ 16%
Data Governance â€“ 9%

ğ€ğ¬ğ¬ğğ¬ğ¬ğ¦ğğ§ğ­ ğƒğğ­ğšğ¢ğ¥ğ¬

Type: Proctored certification
Total number of questions: 45
Time limit: 90 minutes
Validity period: 2 years

ğğ«ğğ©ğšğ«ğšğ­ğ¢ğ¨ğ§ :

1. Data Engineer Associate Exam Guide: https://lnkd.in/gKwVZHiA
2. Databricks Certified Data Engineer Associate - Preparation
https://lnkd.in/gyz3Fdhi
3. Practice Exams: Databricks Certified Data Engineer Associate
https://lnkd.in/gB9FYqbP
4. Instructor-led Training : https://lnkd.in/gu8uzk5b

You can consider following Derar Alhussein who teaches databricks examinations .

Here I am sharing Data Engineering Cert Prep - Notes, that should help you in your preparation.

```
https://www.coursera.org/specializations/data-science-with-databricks-for-data-analysts

https://www.reddit.com/r/databricks/comments/1eketdw/i_created_a_free_databricks_certificate_questions/

https://docs.google.com/spreadsheets/d/1X0bUPGZFap1UWQsQRK4rdMDdANzRp0oVIssV6KRLkxI/edit?pli=1&gid=0#gid=0

https://www.certfun.com/databricks/databricks-certified-machine-learning-associate

### PySpark in 2023

https://www.databricks.com/blog/pyspark-2023-year-review

### Databricks technical blogs
https://community.databricks.com/t5/technical-blog/bg-p/technical-blog

### Databricks SQL:

https://www.youtube.com/watch?v=yeV-dTX7QRo

ğŸ‘©â€ğŸ’»Databricks has released 6 FREE self-paced courses to master Data & AI, including our new Generative AI course!ğŸ”¥ ğŸ™Œ 
No payments required 

ğŸ‘‰ Link to DB Academy: https://lnkd.in/gzkWqHMu

ğŸŠ ğŸ“£ On the top of that, the Learning festival is back! 
```
From the 10th July to the 24th July 2024, a 50% off Databricks certification voucher
(worth US$100) and a 20% discount coupon for Databricks Academy Labs will be given to the users who complete at least one of the below 6 courses within the duration of the virtual festival (ğŸ“…i.e. 10 July 2024 - 24 July 2024). 

Here are the 6 courses that you don't want to miss in 2024:
1ï¸âƒ£ Data Engineer Learning Plan - Data Engineering with Databricks
2ï¸âƒ£ Data Engineer Learning Plan - Advanced Data Engineering with Databricks
3ï¸âƒ£ Data Analyst Learning Plan - Data Analysis with Databricks SQL
4ï¸âƒ£ Machine Learning Practitioner Learning Plan - ML with Databricks
5ï¸âƒ£ Machine Learning Practitioner Learning Plan - ML in Production
7ï¸âƒ£ Generative AI Engineering Pathway - Generative AI Engineer
```
### Data observbility
https://www.montecarlodata.com/blog-introducing-the-5-pillars-of-data-observability/
```
Is the data up-to-date?
Is the data complete?
Are fields within expected ranges?
Is the null rate higher or lower than it should be?
Has the schema changed?
```

https://habr.com/ru/articles/822669/ Ğ¥Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ±Ğ·Ğ¾Ñ€ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

### Slowly changing dimensions

https://en.wikipedia.org/wiki/Slowly_changing_dimension

1. â€œSCD Type 1â€ â€” the â€œOld Recordâ€ is â€œOverwrittenâ€ with the â€œNew Recordâ€ (MERGE/UPSERT)
2. â€œSCD Type 2â€ â€” a â€œNew Recordâ€ is â€œIntroducedâ€ for â€œEach Changeâ€ of the â€œAttributeâ€.
3. â€œSCD Type 3â€ â€” a â€œNew Columnâ€ is â€œIntroducedâ€ for â€œEach Changeâ€ of the â€œAttributeâ€.
```
MERGE INTO training.person TARGET
USING vw_person SOURCE
ON TARGET.PersonId = SOURCE.PersonId
WHEN MATCHED THEN
  UPDATE SET
    TARGET.FirstName = SOURCE.FirstName,
    TARGET.LastName = SOURCE.LastName,
    TARGET.Country = SOURCE.Country
WHEN NOT MATCHED THEN
  INSERT
  (
    PersonId,
    FirstName,
    LastName,
    Country
  )
  VALUES
  (
    SOURCE.PersonId,
    SOURCE.FirstName,
    SOURCE.LastName,
    SOURCE.Country
  )
```  
## Databricks
https://overcast.blog/databricks-cost-optimization-a-practical-guide-d609a9cee9ec
```
DBUs (Databricks Units)

A Databricks Unit (DBU) is the billing unit used to measure compute power per hour.
Its cost depends on factors like the workload type, instance, and runtime used.
For example, an All-Purpose Compute cluster typically costs around $0.40 per DBU per hour for a standard runtime.

What is a DBU?
A DBU measures resources consumed by a workload in seconds.
The number of DBUs varies based on cluster type (e.g., All-Purpose, Jobs Light, SQL) and premium options like Photon or GPU instances. Databricks charges DBUs from the moment a cluster starts, even when idle.
Configuring auto-termination settings helps avoid paying for unused resources.
```

https://www.youtube.com/@Databricks

https://habr.com/ru/companies/slurm/articles/754464/ Apache Spark 3.4 Ğ´Ğ»Ñ Databricks Runtime 13.0

https://habr.com/ru/companies/otus/articles/592605/

https://habr.com/ru/companies/slurm/articles/754464/

https://www.youtube.com/watch?v=BmhlAf3pk84 Databricks Certified Data Engineer Associate Exam Preparation - 45 Practice Questions & Answers
 
https://www.youtube.com/watch?v=Kn_Gu_3DCeE    REST API in Databricks 

```
The Databricks lakehouse architecture combines data stored with the Delta Lake protocol in cloud object storage with metadata registered to a metastore.
There are five primary objects in the Databricks lakehouse:

Catalog: a grouping of databases.
Database or schema: a grouping of objects in a catalog. Databases contain tables, views, and functions.
Table: a collection of rows and columns stored as data files in object storage.
View: a saved query typically against one or more tables or data sources.
Function: saved logic that returns a scalar value or set of rows.
```
#### Unity Catalog

https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/
```
Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Azure Databricks workspaces.

In Unity Catalog, the hierarchy of primary data objects flows from metastore to table or volume:

-  Metastore: The top-level container for metadata. Each metastore exposes a three-level namespace (catalog.schema.table) that organizes your data.
-  Catalog: The first layer of the object hierarchy, used to organize your data assets.
-  Schema: Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views.
- Tables, views, and volumes: At the lowest level in the data object hierarchy are tables, views, and volumes. Volumes provide governance for non-tabular data.
- Models: Although they are not, strictly speaking, data assets, registered models can also be managed in Unity Catalog and reside at the lowest level in the object hierarchy.
```

#### Metastore
```
The metastore contains all of the metadata that defines data objects in the lakehouse. Azure Databricks provides the following metastore options:

Unity Catalog metastore: Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilitie
Built-in Hive metastore (legacy): Each Azure Databricks workspace includes a built-in Hive metastore as a managed service.
External Hive metastore (legacy): 
```
### Medallion architecture
https://learn.microsoft.com/en-us/azure/databricks/lakehouse/medallion
```
The terms bronze (raw), silver (validated), and gold (enriched) describe the quality of the data in each of these layers.

 The silver layer represents a validated, enriched version of our data that can be trusted for downstream analytics.

This gold data is often highly refined and aggregated, containing data that powers analytics, machine learning, and production applications. 
```
### Timetravel

https://learn.microsoft.com/en-us/azure/databricks/delta/history
```
DESCRIBE HISTORY '/data/events/'          -- get the full history of the table
DESCRIBE HISTORY delta.`/data/events/`
DESCRIBE HISTORY '/data/events/' LIMIT 1  -- get the last operation only

DESCRIBE HISTORY eventsTable
RESTORE TABLE db.target_table TO VERSION AS OF <version>
RESTORE TABLE delta.`/data/target/` TO TIMESTAMP AS OF <timestamp>

SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'
SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123
```

#### Tables
```
There are two kinds of tables in Databricks, managed and unmanaged (or external) tables.

__Managed__ tables: Databricks manages both the metadata and the data for a managed table;
when you drop a table, you also delete the underlying data. Data analysts and other users that mostly work in SQL may prefer this behavior.
 Managed tables are the default when creating a table.

it is possible to create tables on Databricks that are not Delta tables. These tables are not backed by Delta Lake, and will not provide the ACID transactions and optimized performance of Delta tables.
Tables falling into this category include tables registered against data in external systems
and tables registered against other file formats in the data lake. 

Note

The Delta Live Tables distinction between live tables and streaming live tables is not enforced from the table perspective.

Delta tables ensure that all data operations (insert, update, delete) adhere to ACID principles â€”
atomicity, consistency, isolation, and durability.


__Unmanaged__ tables will always specify a LOCATION during table creation;
 you can either register an existing directory of data files as a table or
 provide a path when a table is first defined.
Because data and metadata are managed independently, you can rename a table
or register it to a new database without needing to move any data.
Data engineers often prefer unmanaged tables and the flexibility they provide for production data.

```
#### Delta Live Tables

https://learn.microsoft.com/en-us/azure/databricks/delta-live-tables/sql-ref

https://learn.microsoft.com/en-us/azure/databricks/delta-live-tables/expectations

```
Delta Live Tables is a declarative framework for building reliable, maintainable, and testable data processing pipelines.
You define the transformations to perform on your data and
 Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.
```
https://www.youtube.com/watch?v=5E65mE_IiNQ Overview on Databricks Delta Live Tables with Multi-Hop Architecture

### Streaming
```
Databricks recommends using Delta Live Tables with Enhanced Autoscaling for streaming workloads. 
See Optimize the cluster utilization of Delta Live Tables pipelines with Enhanced Autoscaling.

Databricks recommends using Auto Loader to ingest supported file types from cloud object storage into Delta Lake.
For ETL pipelines, Databricks recommends using Delta Live Tables (which uses Delta tables and Structured Streaming).
You can also configure incremental ETL workloads by streaming to and from Delta Lake tables.

What is the difference between Streaming live table and live table?

A live table or view always reflects the results of the query that defines it,
including when the query defining the table or view is updated, or an input data source is updated.
 Like a traditional materialized view, a live table or view may be entirely computed when possible to optimize computation resources and time.

A streaming live table or view processes data that has been added only since the last pipeline update.
Streaming tables and views are stateful; if the defining query changes, new data will be processed based on the new query and existing data is not recomputed.
```
#### Liquid clustering
https://docs.databricks.com/en/delta/clustering.html
```
Delta Lake liquid clustering replaces table partitioning and ZORDER to simplify data layout decisions and optimize query performance.
Databricks recommends using Databricks Runtime 15.2 and above for all tables with liquid clustering enabled.
Clustering is not compatible with partitioning or ZORDER,
and requires that you use Databricks to manage all layout and optimization operations for data in your table.
After liquid clustering is enabled, run OPTIMIZE jobs as usual to incrementally cluster data.

CREATE TABLE table1(col0 int, col1 string) USING DELTA CLUSTER BY (col0);

ALTER TABLE <table_name> CLUSTER BY (<clustering_columns>)
```

#### Performance improvement wit OPTIMIZE and Z-ordering

https://docs.databricks.com/en/optimizations/predictive-optimization.html

OPTIMIZE table_name [WHERE predicate]
  [ZORDER BY (col_name1 [, ...] ) ]
```
One way to improve this speed is to coalesce small files into larger ones.
In Databricks Runtime 13.3 and above, Databricks __recommends using clustering__ for Delta table layout.
 See Use liquid clustering for Delta tables.
Databricks recommends using predictive optimization to automatically run OPTIMIZE for Delta tables
```

#### Partitioning vs Z-ordering

https://medium.com/@tomhcorbin/data-storage-decisions-partitioning-vs-z-ordering-e39d5cddb178
```
Only partition tables larger than a terabyte in size.
Partitioning tables smaller than this tend not to be worth the overhead.
Try to keep your partition size around 1GB each or larger.
This helps keep the number of partitions lower.
If you regularly query a table based on a certain column,
this table might be a good candidate for partitioning.

When to Z-Order

If you expect a column to be commonly used in query predicates and if that column has high cardinality
(that is, a large number of distinct values) which might make it ineffective for PARTITIONing the table by,
then use ZORDER BY instead 

If your table is less than a terabyte in size, Z-order it instead of partitioning it.
Use Z-ordering when your queries span multiple dimensions.
But donâ€™t donâ€™t Z-order on too many columns.
If you regularly query based on high cardinality columns, use Z-ordering.
 

ZORDER BY Collocatse column information in the same set of files. Co-locality is used by Delta Lake data-skipping algorithms to dramatically reduce the amount of data that needs to be read. You can specify multiple columns for ZORDER BY as a comma-separated list. However, the effectiveness of the locality decreases with each additional column.

You cannot use this clause on tables that use liquid clustering.

 
SQL
   OPTIMIZE events;

  OPTIMIZE events WHERE date >= '2017-01-01';

  OPTIMIZE events
    WHERE date >= current_timestamp() - INTERVAL 1 day
    ZORDER BY (eventType);
```

https://www.youtube.com/watch?v=A1aR1A8OwOU Z-Order

### Databricks supports parameterized queries

https://www.reddit.com/r/databricks/comments/1eg59uq/databricks_supports_parameterized_queries/#lightbox

df.createOrReplaceTempView('t1') and using t1 when apparently it's not required

### Datalake

https://docs.databricks.com/en/delta/table-properties.html Delta table properties

https://docs.databricks.com/en/delta/data-skipping.html
```
In Databricks Runtime 13.3 and above,
Databricks recommends using clustering for Delta table layout. Clustering is not compatible with Z-ordering.
See Use liquid clustering for Delta tables.
```
https://www.youtube.com/watch?v=dJvdzJQHpaQ

https://www.youtube.com/watch?v=u95h8bBWpjI


Apache Spark 3.4   Ğ²Ğ¾ÑˆĞµĞ» Ğ² Ñ€ĞµĞ»Ğ¸Ğ· Databricks Runtime 13.0.

Ğ’ Apache Spark 3.4 Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ¸ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Protobuf Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ from_protobuf() Ğ¸ to_protobuf().

```
 Ğ”Ğ¾ Ğ²ĞµÑ€ÑĞ¸Ğ¸ 3.4 Ğ² Dataset API Apache Spark Ğ±Ñ‹Ğ» Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ PIVOT, Ğ½Ğ¾ Ğ½Ğµ ĞµĞ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ MELT.
Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ¾Ğ½Ğ° Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ DataFrame Ğ¸Ğ· ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°,
ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ PIVOT, Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚, Ğ¿Ğ¾ Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹-Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹.
Ğ­Ñ‚Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğº groupBy(...).pivot(...).agg(...), Ğ·Ğ° Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°.
Ğ­Ñ‚Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ DataFrame Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñƒ,
Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹ ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°Ğ¼Ğ¸-Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ğ° Ğ²ÑĞµ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹ ("Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ") "Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ" Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞ¸,
Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ²Ğ° Ğ½ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğº, ĞºĞ°Ğº ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¾.
```
#### Export CSV file locally
```
f="dbfs:/FileStore/uploads/mlubinsky/23_apps_review_count.csv"
_sqldf.write.option("header",True).csv(f)
```
Issue with code above :  it will create folder, ponentially with > 1 file

Solution - convert to pandas:
```
fname="FileStore/uploads/mlubinsky/icon_mapping_march_29.csv"
output_fname = "/dbfs/"+fname
pd = mapping_all.toPandas()
#pd.to_csv(output_fname) # this will create index
pd.to_csv(output_fname, index=False) # no index

print("mapping_all.count()=", mapping_all.count())

read_fname="dbfs:/"+fname
df_read = spark.read.option("header", "true").format("csv").load(read_fname)
print("df_read.count()=", df_read.count())
df_read.display()

Yet another way to read csv file from DBFS
def load_icon_mapping_from_dbfs(spark, params):
        path = "/FileStore/tables/IAP/Icon_mappings/IAP_Icon_map_v10.csv"
        df_csv = spark.read.option("header", "true").format("csv").load(path)
        print(icon_mapping.count())
        return df_csv

```
Check result:
```
%sh
ls /dbfs/FileStore/uploads/mlubinsky/icon_mapping_march_29.csv

```
#### How to download file from DBFS to local machine

https://app-annie-p-dom-aio.cloud.databricks.com/files/uploads/mlubinsky/icon_mapping_march_29.csv?o=3836794479208844

https://app-annie-p-mkt-derivatives.cloud.databricks.com/files/uploads/anna/paid_search.csv?o=651224624067749

https://app-annie-p-dom-aio.cloud.databricks.com/files/tables/IAP/Icon_mappings/IAP_Icon_map_v10.csv?o=3836794479208844

 


just change the part after o=


### Generate sequence
```
SELECT explode( sequence(DATE'2018-02-01', DATE'2018-02-09', INTERVAL 1 DAY) ) as generated_date
```

###  consider PIVOT, UNPIVOT

```
WITH G as
(  
SELECT explode( sequence(DATE'2022-08-01', DATE'2022-11-30', INTERVAL 1 DAY) ) as generated_date
)
,
daily_reviews as ( 
SELECT
  app_id, process_date, count(*) as daily_reviews_cnt
from parquet.`s3://b2c-prod-advanced-review/fact/REVIEW_UNIFORM/version=1.0.5/`
where process_date > '2022-08-01' and process_date <=  '2022-11-30'
-- and ( app_id = 20600000009072 or   app_id =  20600000000465 )
group by app_id, process_date
),

popular_apps as
(
 select app_id, 
 max(daily_reviews_cnt) as max_daily_reviews_cnt,
 sum(daily_reviews_cnt) as sum_daily_reviews_cnt
 from daily_reviews
 group by app_id
 having max(daily_reviews_cnt) > 100
)
,
g_cross_app as
(
SELECT G.generated_date, popular_apps.app_id, 
popular_apps.max_daily_reviews_cnt,
popular_apps.sum_daily_reviews_cnt
FROM G 
CROSS join popular_apps
)
,
gaps as (
SELECT 
G.generated_date,  
G.app_id, 
G.max_daily_reviews_cnt,
G.sum_daily_reviews_cnt,
r.process_date,
r.daily_reviews_cnt
FROM  g_cross_app as G LEFT JOIN 
daily_reviews as r ON G.generated_date = r.process_date AND G.app_id = r.app_id
where  r.process_date IS NULL
order by G.generated_date
),
result as
( 
SELECT 
M.app_id, 
M.app_name,
gaps.sum_daily_reviews_cnt,
array_join(sort_array(collect_list(generated_date)), ', ') as dates_without_reviews

FROM gaps
JOIN  APP_NAMES_view AS M ON   gaps.app_id = M.app_id
group by M.app_id, M.app_name,
gaps.sum_daily_reviews_cnt
 
)
SELECT * FROM result
order by sum_daily_reviews_cnt DESC
```

### Delta format

https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/

```
from bdp.common.spark.spark_utils import load_as_spark_df
import pyspark.sql.functions as F

path= "s3://b2c-prod-dca-model-evaluate/oss/MPS_APPIQ_TAXONOMY_WEIGHTS_DS/version=0.3/"
aio_weights = spark.read.parquet(path)
aio_weights = aio_weights.withColumn("version",F.lit("1.1.0"))

params={"version": "1.0.0"}
df_WEIGHTS_100 = load_as_spark_df(spark, "MPS_APPIQ_TAXONOMY_WEIGHTS_O", params)

new_path = "s3://aardvark-prod-dca-data/oss/tmp_MPS_APPIQ_TAXONOMY_WEIGHTS"

#### replaceWhere https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/
aio_weights.write.partitionBy("version").format("delta").save(new_path)
df_WEIGHTS_100.write.format("delta").option("replaceWhere", "version" == "1.0.0").mode("overwrite").save(new_path)
```

### Message:
Determining location of DBIO file fragments. This operation can take some time.

https://docs.databricks.com/optimizations/disk-cache.html

c5d, r5d, and z1d series workers are configured for disk caching, but not enabled by default. To enable for caching, see Enable or disable the disk cache.

spark.databricks.io.cache.enabled true
spark.databricks.io.cache.maxDiskUsage "{DISK SPACE PER NODE RESERVED FOR CACHED DATA}"
spark.databricks.io.cache.maxMetaDataCache "{DISK SPACE PER NODE RESERVED FOR CACHED METADATA}"

https://www.databricks.com/blog/2018/07/31/processing-petabytes-of-data-in-seconds-with-databricks-delta.html

Solution:
%sql Optimize [table name]


How to interact with files on Databricks

https://docs.databricks.com/files/index.html 

%sh <command> /dbfs/<path>/

```
%sh: Allows you to run shell code in your notebook. To fail the cell if the shell command has a non-zero exit status, add the -e option. This command runs only on the Apache Spark driver, and not the workers. 
To run a shell command on all nodes, use an init script.

%fs: Allows you to use dbutils filesystem commands.
For example, to run the dbutils.fs.ls command to list files, 
you can specify %fs ls instead. For more information, see How to interact with files on Databricks.

%md: Allows you to include various types of documentation, including text, images, and mathematical formulas and equations. See the next section.

```

https://www.youtube.com/c/databricks

https://community.cloud.databricks.com/login.html. - playground

https://docs.databricks.com/data/databricks-datasets.html

https://databricks.com/session_na20/patterns-and-operational-insights-from-the-first-users-of-delta-lake

### DB workflow

Workflows > click Create

#### jobs
https://docs.databricks.com/data-engineering/jobs/jobs.html
A job is a way to run non-interactive code in a Databricks cluster. 
For example, you can run an extract, transform, and load (ETL) 

https://docs.databricks.com/data-engineering/jobs/index.html

You create jobs through the Jobs UI, the Jobs API, or the Databricks CLI. 
The Jobs UI allows you to monitor, test, and troubleshoot your running and completed jobs.

https://databricks.com/blog/2022/05/10/introducing-databricks-workflows.html

https://docs.databricks.com/data-engineering/jobs/jobs-quickstart.html


https://www.youtube.com/watch?v=H2FS4ijpFZA


### JDBC

https://databricks.com/spark/jdbc-drivers-archive

### PySpark

https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf

What is the difference:
#### createOrReplaceTempView
https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceTempView.html
```
df=spark.read.format("delta").load("s3://aardvark-prod-dca-data/fact/APP_TOTAL_REVENUE/range_type=WEEK/")
df.createOrReplaceTempView("weekly_table")
spark.sql(SQL).show(55)
```
#### registerTempTable (deprecated)

https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.registerTempTable.html?highlight=registertemptable
```
path="s3://aardvark-prod-dca-data/fact/APP_TOTAL_REVENUE/range_type=WEEK/date=2012-01-14/"
df=spark.read.format("delta").load(path)
df.show(3)

df.registerTempTable("like_table_name")
df_result = spark.sql("select * from like_table_name ")
df_result.show(3)
```



https://dbmstutorials.com/pyspark/spark-overview-and-setup.html

https://sparkbyexamples.com/pyspark-tutorial/

https://sqlandhadoop.com/pyspark-tutorial-distinct-filter-sort-on-dataframe/

https://sqlandhadoop.com/pyspark-filter-25-examples-to-teach-you-everything/

https://www.youtube.com/watch?v=SBTvJU2vEoc&list=PL7_h0bRfL52qWoCcS18nXcT1s-5rSa1yp


```
from pyspark.sql.types import StructType, StructField, IntegerType

schema = StructType([
    StructField("member_srl", IntegerType(), True),
    StructField("click_day", IntegerType(), True),
    StructField("productid", IntegerType(), True)])

df = spark.read.csv("user_click_seq.csv",header=False,schema=schema)
```


Rename columns:
```
column_list=['catid','catgroup','catname','catdesc']
df_category=df_category.toDF(*column_list)
df_category.show(truncate=False)

df_category.columns 
df_category.dtypes  # column names and types
df_category.printSchema()
```

### New staff

https://databricks.com/product/photon

https://datahubproject.io/ - instead Amundsen

greatexpectations.io

### Iceberg

 https://iceberg.apache.org/

 https://blog.det.life/exploring-advanced-open-data-formats-apache-hudi-apache-iceberg-and-delta-lake-eb764c161159

 https://seattledataguy.substack.com/p/apache-iceberg-what-is-it?utm_source=substack

https://www.infoworld.com/article/3479001/why-apache-iceberg-is-on-fire-right-now.html


### DBT

https://blog.det.life/no-data-engineers-dont-need-dbt-30573eafa15e

https://blog.det.life/no-data-engineers-dont-need-dbt-30573eafa15e

https://habr.com/ru/articles/821503/

https://medium.com/apache-airflow/how-we-orchestrate-2000-dbt-models-in-apache-airflow-90901504032d
  
https://www.adventofdata.com/modern-data-modeling-start-with-the-end/

https://medium.com/datamindedbe/use-dbt-and-duckdb-instead-of-spark-in-data-pipelines-9063a31ea2b5

https://docs.getdbt.com/docs/introduction

https://www.youtube.com/watch?v=5rNquRnNb4E

https://www.youtube.com/watch?v=5rNquRnNb4E&list=PLy4OcwImJzBLJzLYxpxaPUmCWp8j1esvT

https://github.com/dbt-labs

### Amundsen or (https://datahubproject.io/ - instead Amundsen)

https://www.amundsen.io/

https://feng-tao.github.io/amundsen/databuilder/

https://github.com/amundsen-io/amundsendatabuilder

https://www.youtube.com/watch?v=GL4DxwCBn60

https://www.astronomer.io/events/recaps/data-lineage-with-openlineage-and-airflow/

https://blog.anant.us/data-engineers-lunch-36-amundsen-dse-with-airflow/
